# Text-To-Image
The goal of this project is to get a better understanding of how text-to-image generation such as in [DALLÂ·E 2](https://openai.com/de-DE/index/dall-e-2/) or similar models work. This work was also inspired by the [Guest video of WelchLabsVideo on the channel of 3Blue1Brown](https://www.youtube.com/watch?v=iv-5mZ_9CPY).

# References
[1] [Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PmLR.](https://arxiv.org/pdf/2103.00020)

[2] [Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2), 3.](https://arxiv.org/abs/2204.06125)

[3] [Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.](https://arxiv.org/abs/2006.11239)
